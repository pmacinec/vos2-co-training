{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-training\n",
    "\n",
    "**Autori:** Peter Macinec, Lukas Janik, Vajk Pomichal, Frantisek Sefcik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zakladne nastavenia a import kniznic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import statistics\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nacitanie datasetu\n",
    "\n",
    "Nase data su dostupne v dvoch suboroch, *train.tsv* a *test.tsv*. Nacitame ich oba a vykoname na nich zakladnu analyzu. Zdroj: https://www.kaggle.com/c/stumbleupon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trenovacie data\n",
    "df = pd.read_csv('data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testovacie data\n",
    "df_t = pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textove atributy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najskor predspracujeme text. Ziskame ho z atributu boilerplate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_content'] = df['boilerplate'].apply(lambda x: json.loads(x)['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz odstranime vsetky znaky, ktore nie su znaky slov. Pouzijeme na to regularne vyrazy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_content'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_content'] = df['body_content'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ako male upravy, aby nas slovnik obsahoval kazde slovo len raz, dame ich vsetky na lowercase a rozdelime texty na slova, aby sme ich nasledne mohli spracovat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_content'] = df['body_content'].apply(lambda x: str(x).lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz potrebujeme este odstranit slova, ktore nedavaju vyznam. O jednom raze prevedieme slova na ich korenovy zaklad pouzitim stemmingu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_content'] = df['body_content'].apply(lambda x: [porter_stemmer.stem(word) for word in x if word not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz uz mame vsetky slova pripravene, uz ich len naspat spojime do jednej suvislej vety, aby s nimi vedeli lahsie pracovat algoritmy spracovania textu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_content_final'] = df['body_content'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(max_features = 1000)\n",
    "tf_idf = tv.fit_transform(df['body_content_final']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atribut URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url_new'] = df['url'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', str(x)))\n",
    "df['url_new'] = df['url_new'].apply(lambda x: str(x).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url_new'] = df['url_new'].apply(lambda x: [porter_stemmer.stem(word) for word in x if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url_final'] = df['url_new'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_url = TfidfVectorizer(max_features = 1000)\n",
    "tf_idf_url = tv_url.fit_transform(df['url_final']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericke atributy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_set = ['avglinksize', 'commonlinkratio_1', 'commonlinkratio_2', 'commonlinkratio_3', 'commonlinkratio_4',\n",
    "                   'hasDomainLink','lengthyLinkDomain','linkwordscore','numberOfLinks',\n",
    "                   'numwords_in_url', 'parametrizedLinkRatio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model\n",
    "\n",
    "Natrenujeme benchmarkovy model, ktory bude natrenovany na vsetkych atributoch. Jeho vysledky sa nasledne budeme snazit dosiahnut s minimom oznacenych dat s co-trainingom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[:, num_feature_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(tf_idf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = pd.concat([df2, pd.DataFrame(tf_idf)], axis=1, join_axes=[df2.index])\n",
    "X_temp.columns = list(range(0,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df1, X_temp], axis=1, join_axes=[df1.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avglinksize</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>commonlinkratio_2</th>\n",
       "      <th>commonlinkratio_3</th>\n",
       "      <th>commonlinkratio_4</th>\n",
       "      <th>hasDomainLink</th>\n",
       "      <th>lengthyLinkDomain</th>\n",
       "      <th>linkwordscore</th>\n",
       "      <th>numberOfLinks</th>\n",
       "      <th>numwords_in_url</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.055556</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.157927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.677966</td>\n",
       "      <td>0.508021</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>187</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.031715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.382883</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.321705</td>\n",
       "      <td>0.120155</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>258</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.543103</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.676471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02762</td>\n",
       "      <td>0.107720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129191</td>\n",
       "      <td>0.128223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088102</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   avglinksize  commonlinkratio_1  commonlinkratio_2  commonlinkratio_3  \\\n",
       "0     2.055556           0.676471           0.205882           0.047059   \n",
       "1     3.677966           0.508021           0.288770           0.213904   \n",
       "2     2.382883           0.562016           0.321705           0.120155   \n",
       "3     1.543103           0.400000           0.100000           0.016667   \n",
       "4     2.676471           0.500000           0.222222           0.123457   \n",
       "\n",
       "   commonlinkratio_4  hasDomainLink  lengthyLinkDomain  linkwordscore  \\\n",
       "0           0.023529              0                  1             24   \n",
       "1           0.144385              0                  1             40   \n",
       "2           0.042636              0                  1             55   \n",
       "3           0.000000              0                  0             24   \n",
       "4           0.043210              0                  1             14   \n",
       "\n",
       "   numberOfLinks  numwords_in_url  ...   1990     1991      1992  1993  \\\n",
       "0            170                8  ...    0.0  0.00000  0.157927   0.0   \n",
       "1            187                9  ...    0.0  0.00000  0.031715   0.0   \n",
       "2            258               11  ...    0.0  0.00000  0.000000   0.0   \n",
       "3            120                5  ...    0.0  0.00000  0.000000   0.0   \n",
       "4            162               10  ...    0.0  0.02762  0.107720   0.0   \n",
       "\n",
       "       1994      1995  1996  1997      1998  1999  \n",
       "0  0.000000  0.028198   0.0   0.0  0.064583   0.0  \n",
       "1  0.000000  0.000000   0.0   0.0  0.000000   0.0  \n",
       "2  0.000000  0.000000   0.0   0.0  0.000000   0.0  \n",
       "3  0.000000  0.000000   0.0   0.0  0.000000   0.0  \n",
       "4  0.129191  0.128223   0.0   0.0  0.088102   0.0  \n",
       "\n",
       "[5 rows x 2011 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names1 = list(range(0,1000))\n",
    "col_names1 = num_feature_set + col_names1\n",
    "col_names2 = list(range(1000,2000))\n",
    "# col_names2 = [str(i) for i in col_names2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_clf = RandomForestClassifier(n_estimators=500, max_depth=20,\n",
    "                              random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_clf.fit(X_train, y_train)\n",
    "y_pred = benchmark_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[656,  74],\n",
       "       [221, 528]], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8005409060175794"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedClassifier:\n",
    "    \n",
    "    def __init__(self,clf1, clf2, cols1, cols2):\n",
    "        self.clf1 = clf1\n",
    "        self.clf2 = clf2\n",
    "        self.cols1 = cols1\n",
    "        self.cols2 = cols2\n",
    "        \n",
    "    def predict(self, df):\n",
    "        result = []\n",
    "        for ind, row in df.iterrows():\n",
    "            \n",
    "            prob0 = self.clf1.predict_proba([df.loc[ind,self.cols1]])[0][0] * self.clf2.predict_proba([df.loc[ind,self.cols2]])[0][0]  \n",
    "            prob1 = self.clf1.predict_proba([df.loc[ind,self.cols1]])[0][1] * self.clf2.predict_proba([df.loc[ind,self.cols2]])[0][1]\n",
    "            res = 0 if prob0 > prob1 else 1\n",
    "            result.append(res)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-training with iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_training(L, U, col_names1, col_names2, clf1, clf2,u=75, k=30,p=1,n=3, label='label', X_test=None, y_test=None, random_state=None):\n",
    "    '''\n",
    "    L - labeled data\n",
    "    U - unlabeled data\n",
    "    col_names1 - columns for clf1\n",
    "    col_names2 - columns for clf2\n",
    "    clf1 - classificator1\n",
    "    clf2 - classificator2\n",
    "    u - number of unlabeled data for training\n",
    "    k - number of iteration\n",
    "    p - number of positive examles to add label per iteration\n",
    "    n - number of negative examles to add label per iteration\n",
    "    label - name of column in L whit label\n",
    "    '''\n",
    "    \n",
    "#     U0 = U.sample(u, random_state= random_state)\n",
    "#     U.drop(U0.index, inplace=True)\n",
    "    k0 = k\n",
    "    pred_1 = []\n",
    "    pred_2 = []\n",
    "    while k > 0:\n",
    "        #print(\"iteration: \", k0 - k)\n",
    "        # step 1: Use L to train a classifier h1 that considers only the x1 portion of x\n",
    "        clf1 = clf1.fit(L[col_names1],  L[label])\n",
    "        \n",
    "        # step 2: Use L, to train a classifier h2 that considers only the x2 portion of x\n",
    "        clf2 = clf2.fit(L[col_names2], L[label])\n",
    "        \n",
    "        # step 3: Allow hl to label p positive and n negative examples from U\n",
    "        predicted_prob1 = clf1.predict_proba(U[col_names1])\n",
    "        top_positive1 = predicted_prob1[:,1].argsort()[-p:]\n",
    "        top_negative1 = predicted_prob1[:,0].argsort()[-n:]\n",
    "        \n",
    "        # step 4: Allow hl to label p positive and n negative examples from U\n",
    "        predicted_prob2 = clf2.predict_proba(U[col_names2])\n",
    "        top_positive2 = predicted_prob2[:,1].argsort()[-p:]\n",
    "        top_negative2 = predicted_prob2[:,0].argsort()[-n:]    \n",
    "        \n",
    "        # step 5: Add these self-labeled examples to L\n",
    "        positive_ind = U.iloc[np.unique(np.concatenate((top_positive1,top_positive2))),:].index\n",
    "        negative_ind = U.iloc[np.unique(np.concatenate((top_negative1,top_negative2))),:].index\n",
    "        self_labeled = U.loc[np.unique(np.concatenate((positive_ind,negative_ind))),:]\n",
    "        U.drop(self_labeled.index, inplace=True)\n",
    "        self_labeled.loc[positive_ind, label] = 1\n",
    "        self_labeled.loc[negative_ind, label] = 0\n",
    "        L = pd.concat([L, self_labeled])\n",
    "    \n",
    "        if (X_test is not None and y_test is not None):\n",
    "            pred = clf1.predict(X_test[col_names1])\n",
    "            pred_1.append(accuracy_score(y_test,pred))\n",
    "            \n",
    "            pred = clf2.predict(X_test[col_names2])\n",
    "            pred_2.append(accuracy_score(y_test,pred))\n",
    "\n",
    "        k -= 1\n",
    "    \n",
    "    return [clf1, clf2, pred_1, pred_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names1 = list(range(0,1000))\n",
    "col_names1 = num_feature_set + col_names1\n",
    "col_names2 = list(range(1000,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [[0 for _ in range(3)] for _ in range(2)]\n",
    "\n",
    "for x in range(2):\n",
    "    df_x0 = X_train[y_train == 0].sample(5)\n",
    "    df_x1 = X_train[y_train == 1].sample(5)\n",
    "    \n",
    "    train = pd.concat([df_x0,df_x1])\n",
    "    x_train = X_train.drop(train.index)\n",
    "    train['label'] = y_train.loc[train.index]\n",
    "\n",
    "    clf1 = LogisticRegression()\n",
    "    clf2 = LogisticRegression()\n",
    "\n",
    "    new_1, new_2, scores[x][0], scores[x][1] = co_training(L=train,U=x_train, \n",
    "                               col_names1=col_names1,\n",
    "                               col_names2=col_names2, clf1=clf1, clf2=clf2, k=30, n=2, p=2, X_test=X_test, y_test=y_test)\n",
    "    \n",
    "#     pred = new_1.predict(X_test[col_names1])\n",
    "#     scores[0].append(accuracy_score(y_test, pred))\n",
    "\n",
    "#     pred = new_2.predict(X_test[col_names2])\n",
    "#     scores[1].append(accuracy_score(y_test, pred))\n",
    "\n",
    "    cl = CombinedClassifier(new_1,new_2,col_names1,col_names2)\n",
    "    pred = cl.predict(X_test)\n",
    "    scores[x][2] = accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy1 = []\n",
    "avg_accuracy2 = []\n",
    "model_accuracy1 = []\n",
    "model_accuracy2 = []\n",
    "for i in range(30):\n",
    "    sum_1 = 0 \n",
    "    sum_2 = 0\n",
    "    for j in range(2):\n",
    "        sum_1 += scores[j][0][i]\n",
    "        sum_2 += scores[j][1][i]\n",
    "        if i == 29:\n",
    "            model_accuracy1.append(scores[j][0][i])\n",
    "            model_accuracy2.append(scores[j][1][i])\n",
    "    avg_accuracy1.append(sum_1 / 2)\n",
    "    avg_accuracy2.append(sum_2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e3426b3c88>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_accuracy1)\n",
    "plt.plot(avg_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.5868830290736985\n",
      "Max: 0.6206896551724138\n",
      "Avg: 0.6037863421230562\n",
      "Std: 0.01690331304935766\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \" + str(np.min(model_accuracy1)))\n",
    "print(\"Max: \" + str(np.max(model_accuracy1)))\n",
    "print(\"Avg: \" + str(np.mean(model_accuracy1)))\n",
    "print(\"Std: \" + str(np.std(model_accuracy1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.6991210277214334\n",
      "Max: 0.7288708586883029\n",
      "Avg: 0.7139959432048681\n",
      "Std: 0.01487491548343478\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \" + str(np.min(model_accuracy2)))\n",
    "print(\"Max: \" + str(np.max(model_accuracy2)))\n",
    "print(\"Avg: \" + str(np.mean(model_accuracy2)))\n",
    "print(\"Std: \" + str(np.std(model_accuracy2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.6173089925625422\n",
      "Max: 0.6747802569303584\n",
      "Avg: 0.6460446247464503\n",
      "Std: 0.028735632183908066\n"
     ]
    }
   ],
   "source": [
    "score_combined = []\n",
    "\n",
    "for i in range(2):\n",
    "    score_combined.append(scores[i][2])\n",
    "\n",
    "print(\"Min: \" + str(np.min(score_combined)))\n",
    "print(\"Max: \" + str(np.max(score_combined)))\n",
    "print(\"Avg: \" + str(np.mean(score_combined)))\n",
    "print(\"Std: \" + str(np.std(score_combined)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-training with threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_training_threshold(L, U, col_names1, col_names2, clf1, clf2, label='label', random_state=None, X_test=None, y_test=None, threshold=0.8):\n",
    "    '''\n",
    "    L - labeled data\n",
    "    U - unlabeled data\n",
    "    col_names1 - columns for clf1\n",
    "    col_names2 - columns for clf2\n",
    "    clf1 - classificator1\n",
    "    clf2 - classificator2\n",
    "    label - name of column in L whit label\n",
    "    threshold - threshold of predictions probability\n",
    "    '''\n",
    "    \n",
    "    pred_1 = []\n",
    "    pred_2 = []\n",
    "    while len(U) > 0:\n",
    "        \n",
    "        # step 1: Use L to train a classifier h1 that considers only the x1 portion of x\n",
    "        clf1 = clf1.fit(L[col_names1],  L[label])\n",
    "        \n",
    "        # step 2: Use L, to train a classifier h2 that considers only the x2 portion of x\n",
    "        clf2 = clf2.fit(L[col_names2], L[label])\n",
    "        \n",
    "        # step 3: Allow hl to label p positive and n negative examples from U\n",
    "        predicted_prob1 = clf1.predict_proba(U[col_names1])\n",
    "        top_positive1 = predicted_prob1[:,1].argsort()[-1:] if predicted_prob1[:,1].argsort()[-1:] > threshold else [] \n",
    "        top_negative1 = predicted_prob1[:,0].argsort()[-1:] if predicted_prob1[:,0].argsort()[-1:] > threshold else []\n",
    "        \n",
    "        # step 4: Allow hl to label p positive and n negative examples from U\n",
    "        predicted_prob2 = clf2.predict_proba(U[col_names2]) \n",
    "        top_positive2 = predicted_prob2[:,1].argsort()[-1:] if predicted_prob2[:,1].argsort()[-1:] > threshold else []\n",
    "        top_negative2 = predicted_prob2[:,0].argsort()[-1:] if predicted_prob2[:,0].argsort()[-1:] > threshold else []\n",
    "        \n",
    "        # step 5: Add these self-labeled examples to L\n",
    "        if(np.unique(np.concatenate((top_positive1,top_positive2, top_negative1, top_negative2))).size == 0):\n",
    "            return [clf1,clf2]\n",
    "        positive_ind = U.iloc[np.unique(np.concatenate((top_positive1,top_positive2))),:].index\n",
    "        negative_ind = U.iloc[np.unique(np.concatenate((top_negative1,top_negative2))),:].index\n",
    "        self_labeled = U.loc[np.unique(np.concatenate((positive_ind,negative_ind))),:]\n",
    "        U.drop(self_labeled.index, inplace=True)\n",
    "        self_labeled.loc[positive_ind, label] = 1\n",
    "        self_labeled.loc[negative_ind, label] = 0\n",
    "        L = pd.concat([L, self_labeled])\n",
    "        \n",
    "        if (X_test is not None and y_test is not None):\n",
    "            pred = clf1.predict(X_test[col_names1])\n",
    "            pred_1.append(accuracy_score(y_test,pred))\n",
    "            \n",
    "            pred = clf2.predict(X_test[col_names2])\n",
    "            pred_2.append(accuracy_score(y_test,pred))\n",
    "    \n",
    "    return [clf1, clf2, pred_1, pred_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-93b00bd5575f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     new_1, new_2, scores[x][0], scores[x][1] = co_training_threshold(L=train,U=x_train, \n\u001b[0;32m     15\u001b[0m                                \u001b[0mcol_names1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol_names1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                                col_names2=col_names2, clf1=clf1, clf2=clf2, X_test=X_test, y_test=y_test, threshold=0.8)\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#     pred = new_1.predict(X_test[col_names1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-127-5d45fa5c0acc>\u001b[0m in \u001b[0;36mco_training_threshold\u001b[1;34m(L, U, col_names1, col_names2, clf1, clf2, label, random_state, X_test, y_test, threshold)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# step 2: Use L, to train a classifier h2 that considers only the x2 portion of x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mclf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_names2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# step 3: Allow hl to label p positive and n negative examples from U\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1303\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    915\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores_threshold = [[0 for _ in range(3)] for _ in range(2)]\n",
    "\n",
    "for x in range(2):\n",
    "    df_x0 = X_train[y_train == 0].sample(5)\n",
    "    df_x1 = X_train[y_train == 1].sample(5)\n",
    "    \n",
    "    train = pd.concat([df_x0,df_x1])\n",
    "    x_train = X_train.drop(train.index)\n",
    "    train['label'] = y_train.loc[train.index]\n",
    "\n",
    "    clf1 = LogisticRegression()\n",
    "    clf2 = LogisticRegression()\n",
    "\n",
    "    new_1, new_2, scores[x][0], scores[x][1] = co_training_threshold(L=train,U=x_train, \n",
    "                               col_names1=col_names1,\n",
    "                               col_names2=col_names2, clf1=clf1, clf2=clf2, X_test=X_test, y_test=y_test, threshold=0.8)\n",
    "    \n",
    "#     pred = new_1.predict(X_test[col_names1])\n",
    "#     scores[0].append(accuracy_score(y_test, pred))\n",
    "\n",
    "#     pred = new_2.predict(X_test[col_names2])\n",
    "#     scores[1].append(accuracy_score(y_test, pred))\n",
    "\n",
    "    cl = CombinedClassifier(new_1,new_2,col_names1,col_names2)\n",
    "    pred = cl.predict(X_test)\n",
    "    scores_threshold[x][2] = accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracy1 = []\n",
    "avg_accuracy2 = []\n",
    "model_accuracy1 = []\n",
    "model_accuracy2 = []\n",
    "for i in range(30):\n",
    "    sum_1 = 0 \n",
    "    sum_2 = 0\n",
    "    for j in range(2):\n",
    "        sum_1 += scores_threshold[j][0][i]\n",
    "        sum_2 += scores_threshold[j][1][i]\n",
    "        if i == 29:\n",
    "            model_accuracy1.append(scores_threshold[j][0][i])\n",
    "            model_accuracy2.append(scores_threshold[j][1][i])\n",
    "    avg_accuracy1.append(sum_1 / 2)\n",
    "    avg_accuracy2.append(sum_2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_accuracy1)\n",
    "plt.plot(avg_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min: \" + str(np.min(model_accuracy1)))\n",
    "print(\"Max: \" + str(np.max(model_accuracy1)))\n",
    "print(\"Avg: \" + str(np.mean(model_accuracy1)))\n",
    "print(\"Std: \" + str(np.std(model_accuracy1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min: \" + str(np.min(model_accuracy2)))\n",
    "print(\"Max: \" + str(np.max(model_accuracy2)))\n",
    "print(\"Avg: \" + str(np.mean(model_accuracy2)))\n",
    "print(\"Std: \" + str(np.std(model_accuracy2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_combined = []\n",
    "\n",
    "for i in range(2):\n",
    "    score_combined.append(scores_threshold[i][2])\n",
    "\n",
    "print(\"Min: \" + str(np.min(score_combined)))\n",
    "print(\"Max: \" + str(np.max(score_combined)))\n",
    "print(\"Avg: \" + str(np.mean(score_combined)))\n",
    "print(\"Std: \" + str(np.std(score_combined)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import classifiers\n",
    "from classifiers import CoTrainingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x0 = X_train[y_train == 0].sample(5)\n",
    "# df_x1 = X_train[y_train == 1].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([df_x0,df_x1])\n",
    "# X_train.drop(train.index, inplace=True)\n",
    "# train['label'] = y_train.loc[train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = set(y_train[y_train == 0].sample(5).index) | set(y_train[y_train == 1].sample(5).index)\n",
    "tmp2 = set(y_train.index) - set(tmp1)\n",
    "y_train.loc[tmp2] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([7227, 180, 1377, 1836, 2066, 4897, 5355, 938, 1059, 1585], dtype='int64')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train != -1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_co_clf = CoTrainingClassifier(LogisticRegression(), p=2, n=2)\n",
    "lg_co_clf.fit(X_train[col_names1].reset_index(), X_train[col_names2].reset_index(), y_train.reset_index())\n",
    "y_pred = lg_co_clf.predict(X_test[col_names1], X_test[col_names2])\n",
    "print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_co_training(df, cols1, cols2, label, clf1, clf2, p=5, n=5, p_class=1, n_class=0):\n",
    "\n",
    "    #toto tu zbytocne rozbijam a spajam, na vstupe mozno lepsie dat series a to do dataframe\n",
    "    df1 = df[[cols1]]\n",
    "    df2 = df[[cols2]]\n",
    "    y= df[label]\n",
    "    \n",
    "    X = pd.concat([df1, df2], axis=1, join_axes=[df1.index])\n",
    "    X.columns = list(range(0,len(X)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    df_x0 = X_train[y_train == n_class].sample(n)\n",
    "\n",
    "    df_x1 = X_train[y_train == p_class].sample(p)\n",
    "\n",
    "    train = pd.concat([df_x0,df_x1])\n",
    "\n",
    "    X_train.drop(train.index, inplace=True)\n",
    "\n",
    "    train['label'] = y_train.loc[train.index]\n",
    "    \n",
    "    \n",
    "    new_1, new_2 = co_training(L=train,U=X_train, \n",
    "                           col_names1=col_names1,\n",
    "                           col_names2=col_names2, clf1=clf1, clf2=clf2, k=30, p=2,n=2)\n",
    "    \n",
    "    # accuracy z new1 a new2 (mozno aj zo spojeneho) trebu ulozit do pola a ten return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
